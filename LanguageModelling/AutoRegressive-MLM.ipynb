{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLM_MLM_TLM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qzJ6BdI5brF6",
        "LOUY1Ig2uzoB",
        "aQzBLQCVu1ky"
      ],
      "authorship_tag": "ABX9TyMLYaIiL+8dix9CHIiMi+9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deterministic-algorithms-lab/NLP-Journey/blob/main/LanguageModelling/CLM_MLM_TLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmUfNCoEbSXE"
      },
      "source": [
        "!git clone https://github.com/deterministic-algorithms-lab/NLP-Journey\n",
        "%cd NLP-Journey\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQKXqR7bt0m"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from haiku.data_structures import to_immutable_dict\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai8N55L2cVlp"
      },
      "source": [
        "import src.DataLoaders.tfds as tfdl\n",
        "from src.Tokenizers.tree_tokenizer import Tree_Tokenizer\n",
        "from src.model.transformer import TransformerFeaturizer, ExtendedEncoder\n",
        "from src.optimizers.adam import get_adam_opt\n",
        "from src.Tokenizers.masking_utils import mask_batch_mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzJ6BdI5brF6"
      },
      "source": [
        "## Setting Up Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Bgmf_3cr4i"
      },
      "source": [
        "config = {\n",
        "          #Data Parameters\n",
        "          'max_length' : 512, \n",
        "          'featurizer_batch_size' : 4,\n",
        "          'mlm_batch_size' : 4,\n",
        "          'data_files' : ['heldout_period_data.jsonlist'],\n",
        "\n",
        "          #Model Parameters\n",
        "          'intermediate_size' : 3072,\n",
        "          'n_heads' : 12,\n",
        "          'n_layers' : 12,\n",
        "          'hidden_size' : 768,\n",
        "          'd_model' : 768,                                                      #same as hidden_size\n",
        "          \n",
        "          #Embeddings Parameters\n",
        "          'embed_dropout_rate' : 0.1,\n",
        "          'lang2id' : {'en' : 1, 'ne' : 2},\n",
        "          \n",
        "          #MHA parameters\n",
        "          'attention_drop_rate' : 0.1,\n",
        "          \n",
        "          #MLP parameters\n",
        "          'fully_connected_drop_rate' : 0.1,\n",
        "          \n",
        "          #Training Parameters\n",
        "          'learning_rate' : 1e-5,\n",
        "          'max_grad_norm' : 1.0,\n",
        "          'l2' : 0.1,\n",
        "          'n_epochs' : 5,\n",
        "          'n_examples' : 25000,\n",
        "\n",
        "          #Task no.\n",
        "          'mlm' : 0,\n",
        "          'clm' : 1,\n",
        "          }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUY1Ig2uzoB"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmC0IPQSbmF4"
      },
      "source": [
        "data_loader = load_reddit_data(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQzBLQCVu1ky"
      },
      "source": [
        "## Training Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl2jwUcauODk"
      },
      "source": [
        "def get_sentences():\n",
        "    for tree in data_loader.tree_generator():\n",
        "        yield tree['title'] + ' ' + tree['selftext']\n",
        "        for id, comment in tree['comments']:\n",
        "            yield comment['body']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oWL0v1ffEus"
      },
      "source": [
        "lm_tokeniser = Tree_Tokenizer(config)\n",
        "lm_tokeniser.train_tokenizer(str_iterator=get_sentences())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCuyUhY4uuOg"
      },
      "source": [
        "print(lm_tokeniser.tokenizer.get_vocab())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw8KWNrKzIf0"
      },
      "source": [
        "### Updating Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZodbGMNEwrrI"
      },
      "source": [
        "config['vocab_size'] = lm_tokeniser.tokenizer.get_vocab_size()\n",
        "\n",
        "#Tokenization ids  \n",
        "config['mask_id'] = lm_tokeniser.tokenizer.token_to_id(\"<mask>\")\n",
        "config['pad_id'] = lm_tokeniser.tokenizer.token_to_id(\"<pad>\")\n",
        "config['sos_id'] = lm_tokeniser.tokenizer.token_to_id(\"<s>\")\n",
        "config['eos_id'] = lm_tokeniser.tokenizer.token_to_id(\"</s>\")\n",
        "config = hk.data_structures.to_immutable_dict(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Purifying the Model Functions and Getting Parameters"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "@jax.jit\n",
        "def featurizer(token_ids, training=True):\n",
        "    features = TransformerFeaturizer(config)(token_ids, training=training)\n",
        "    return features\n",
        "\n",
        "@jax.jit\n",
        "def logits_fn(comment_embds, comment_mask, masked_token_ids, training=True):\n",
        "    logits = ExtendedEncoder(config)(comment_embds, comment_mask, \n",
        "                                     masked_token_ids, training=training)\n",
        "    return logits\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "pure_logits_fn = hk.transform(logits_fn)\n",
        "pure_featurizer_fn = hk.transform(featurizer)\n",
        "\n",
        "comment_encoding = lm_tokeniser.batch_encode_plus(['sample sentence']*config['featurizer_batch_size'])\n",
        "token_encoding = lm_tokeniser.batch_encode_plus(['sample sentence']*config['mlm_batch_size'])\n",
        "\n",
        "token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "comment_ids = np.asarray(lm_tokeniser.get_token_ids(comment_encoding), dtype=np.int16)\n",
        "\n",
        "masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, subkey = jax.random.split(key)\n",
        "featuirzer_params = pure_featurizer_fn.init(subkey, comment_ids, training=True)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "comment_embds = pure_featurizer_fn.apply(featurizer_params, subkey, commnent_ids, training=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "comment_embds = jnp.transpose( jnp.tile(comment_embds, config['max_length']), (1,0,2) )\n",
        "comment_mask = jnp.ones_like(comment_embds[:,:,0])\n",
        "\n",
        "ExtendedEncoder_params = pure_logits_fn.init(subkey, comment_embds, \n",
        "                                             comment_mask, masked_token_ids,\n",
        "                                             training=training)\n",
        "\n",
        "params = to_immutable_dict( {'comments_encoder' : featurizer_params, \n",
        "                             'mlm_predictor' : ExtendedEncoder_params } )"
      ]
    },
    {
      "source": [
        "## Running Model and Getting Loss"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy(config, original_batch, logits, masked_token_ids):\n",
        "    logits_mask = (masked_token_ids==config['mask_id'])\n",
        "    logits = jax.vmap(jnp.multiply, (None,2), 2)(logits_mask,logits)\n",
        "    labels = hk.one_hot(original_batch, config['vocab_size'])\n",
        "    softmax_xent = -jnp.sum(labels*jax.nn.log_softmax(logits))\n",
        "    total_masks = jnp.sum(logits_mask)\n",
        "    softmax_xent /= total_masks\n",
        "    return softmax_xent\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(params, key, tree, config):\n",
        "    \"\"\"\n",
        "    Calculates loss for all nodes of a single tree.\n",
        "    The masked tokens of each location in a comment are predicted \n",
        "    conditioned on the embeddings of all the parent comments.\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    #Prepare embeddings of each comment\n",
        "    empty_elem = jnp.asarray([config['pad_id']]*config['max_length'], dtype=jnp.int16)\n",
        "    batches = tree_to_batch(tree, config['featurizer_batch_size'], empty_elem = empty_elem)\n",
        "    encodings = []\n",
        "    for batch in batches:\n",
        "        key, subkey = jax.random.split(key)\n",
        "        features = pure_featurizer_fn.apply(subkey, params['comments_encoder'],\n",
        "                                            batch, training=True)\n",
        "        encodings.append(features)\n",
        "    tree = batch_to_tree(tree, encodings, config['featurizer_batch_size'])\n",
        "\n",
        "    #Calculate loss for each masked position in each comment.\n",
        "    comment_batches = tree_to_batch(tree, config['mlm_batch_size'], key=None, empty_elem={})\n",
        "    empty_elem = jnp.asarray([0]*config['d_model'], dtype=jnp.int16)\n",
        "    \n",
        "    for original_batch, comment_batch in zip(batches, comment_batches):\n",
        "        parent_comment_embds, mask_for_embds = gather_batch_parents(tree, comment_batch, \n",
        "                                                                    config['max_length'], key='comment_embds', \n",
        "                                                                    empty_elem=empty_elem)\n",
        "        key, subkey = jax.random.split(key)\n",
        "        masked_batch, original_batch = mask_batch_mlm(subkey, config, original_batch)\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        logits = pure_logits_fn.apply(params['mlm_predictor'], subkey, parent_comment_embds, \n",
        "                                      mask_for_embds, masked_batch, training=True)\n",
        "        loss += cross_entropy(config, original_batch, logits, masked_batch)\n",
        "    \n",
        "    return loss\n"
      ]
    },
    {
      "source": [
        "## Optimizer"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "4C2NgvBPteSs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOUtfkolthMo"
      },
      "source": [
        "opt = get_adam_opt(config)\n",
        "opt_state = opt.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update(opt_state, params, key, tree, config):\n",
        "    batch_loss, grad = jax.value_and_grad(loss)(params, key, tree, config)\n",
        "    updates, opt_state = opt.update(grad, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, batch_loss"
      ]
    },
    {
      "source": [
        "## Training Loop"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = []\n",
        "for step, tree in enumerate(data_loader.tree_generator()):\n",
        "    if step%100==0:\n",
        "        print(f'[Step {step}]')\n",
        "    \n",
        "    tree = lm_tokeniser.tokenize_tree(tree)\n",
        "    \n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state, batch_loss = update(opt_state, params, subkey,\n",
        "                                           tree, config)\n",
        "    losses.append(batch_loss)\n",
        "\n",
        "    if step%100==0 and step!=0:\n",
        "        print(sum(losses)/100)\n",
        "        losses = []"
      ]
    }
  ]
}