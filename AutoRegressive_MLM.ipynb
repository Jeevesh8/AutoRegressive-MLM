{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoRegressive-MLM.ipynb",
      "provenance": [],
      "mount_file_id": "1fOllMrQcyJOAgrfB3dmvJh88_ju8dyxe",
      "authorship_tag": "ABX9TyO2lWsL3VN8WeSehSlT9bws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/AutoRegressive-MLM/blob/main/AutoRegressive_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Pi2iyFGLaW"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etUtVZ7dGQYY"
      },
      "source": [
        "!git clone https://github.com/Jeevesh8/AutoRegressive-MLM\n",
        "%cd AutoRegressive-MLM\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNAMewDLGYE2"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from haiku.data_structures import to_immutable_dict\n",
        "import optax\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vqsz2nSGflE"
      },
      "source": [
        "from src.DataLoaders.json import load_reddit_data\n",
        "from src.Tokenizers.tree_tokenizer import Tree_Tokenizer\n",
        "from src.model.transformer import TransformerFeaturizer, ExtendedEncoder\n",
        "from src.optimizers.adam import get_adam_opt\n",
        "from src.Tokenizers.masking_utils import mask_batch_mlm\n",
        "from src.Tokenizers.utils import tree_to_batch, batch_to_tree, gather_batch_parents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYlj-stnG0y-"
      },
      "source": [
        "## Setting Up Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H7T144MGnPn"
      },
      "source": [
        "config = {\n",
        "          #Data Parameters\n",
        "          'max_length' : 128, \n",
        "          'featurizer_batch_size' : 4,\n",
        "          'mlm_batch_size' : 4,\n",
        "          'n_epochs' : 10,\n",
        "          'data_files' : ['/content/drive/MyDrive/2SCL/Argumentation/first_batch_data/train_period_data.jsonlist'],\n",
        "          'discourse_markers_file' : '/content/drive/MyDrive/2SCL/Argumentation/first_batch_data/Discourse_Markers.txt',\n",
        "          \n",
        "          #Model Parameters\n",
        "          'intermediate_size' : 256,\n",
        "          'n_heads' : 2,\n",
        "          'n_layers' : 2,\n",
        "          'hidden_size' : 128,\n",
        "          'd_model' : 128,                                                      #same as hidden_size\n",
        "          'max_losses' : 10,                                                    #max. number of losses to backpropagate at once\n",
        "          'max_tree_size' : 60,\n",
        "         \n",
        "          #Embeddings Parameters\n",
        "          'embed_dropout_rate' : 0.1,\n",
        "          \n",
        "          #MHA parameters\n",
        "          'attention_drop_rate' : 0.1,\n",
        "          \n",
        "          #MLP parameters\n",
        "          'fully_connected_drop_rate' : 0.1,\n",
        "          \n",
        "          #Training Parameters\n",
        "          'learning_rate' : 1e-5,\n",
        "          'max_grad_norm' : 1.0,\n",
        "          'l2' : 0.1,\n",
        "\n",
        "           #colab parameter\n",
        "          'restart_from' : 0,\n",
        "          }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Loading Pre-Trained Weights"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from functools import lru_cache\n",
        "\n",
        "import joblib\n",
        "import requests\n",
        "\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "huggingface_roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "\n",
        "huggingface_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def postprocess_key(key):\n",
        "    key = key.replace('model/featurizer/bert/', '')\n",
        "    key = key.replace(':0', '')\n",
        "    key = key.replace('self/', '')\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def get_pretrained_weights():\n",
        "    # We'll use the weight dictionary from the Roberta encoder at \n",
        "    # https://github.com/IndicoDataSolutions/finetune\n",
        "    remote_url = \"https://bendropbox.s3.amazonaws.com/roberta/roberta-model-sm-v2.jl\"\n",
        "    weights = joblib.load(BytesIO(requests.get(remote_url).content))\n",
        "\n",
        "    weights = {\n",
        "        postprocess_key(key): value\n",
        "        for key, value in weights.items()\n",
        "    }\n",
        "\n",
        "    input_embeddings = huggingface_roberta.get_input_embeddings()\n",
        "    weights['embeddings/word_embeddings'] = input_embeddings.weight.detach().numpy()\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config['pretrained'] = get_pretrained_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mTGRKFOG5ym"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1IwWGFqG3Tj"
      },
      "source": [
        "data_loader = load_reddit_data(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTnb-HWHG9sq"
      },
      "source": [
        "## Training Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwkaFvYOG75v"
      },
      "source": [
        "def get_sentences():\n",
        "    for tree in data_loader.tree_generator():\n",
        "        yield tree['title'] + ' ' + tree['selftext']\n",
        "        for id, comment in tree['comments'].items():\n",
        "            yield comment['body']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19xCsZgwG_qo"
      },
      "source": [
        "lm_tokeniser = Tree_Tokenizer(config)\n",
        "lm_tokeniser.train_tokenizer(str_iter=get_sentences())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Or Load Pre-Trained Tokenizer"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lm_tokeniser = Tree_Tokenizer(config)\n",
        "lm_tokeniser.tokenizer = huggingface_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2qLvVaEHA8s",
        "outputId": "11da323b-61b1-4763-f761-4588eaa0813a"
      },
      "source": [
        "print(lm_tokeniser.tokenizer.get_vocab())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVFOIrIFUrS5"
      },
      "source": [
        "### Updating Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PWLW7JlUmo9"
      },
      "source": [
        "config['vocab_size'] = lm_tokeniser.tokenizer.get_vocab_size()\n",
        "\n",
        "#Tokenization ids  \n",
        "config['mask_id'] = lm_tokeniser.tokenizer.token_to_id(\"<mask>\")\n",
        "config['pad_id'] = lm_tokeniser.tokenizer.token_to_id(\"<pad>\")\n",
        "config['sos_id'] = lm_tokeniser.tokenizer.token_to_id(\"<s>\")\n",
        "config['eos_id'] = lm_tokeniser.tokenizer.token_to_id(\"</s>\")\n",
        "config['total_steps'] = len([0 for tree in data_loader.tree_generator()])\n",
        "config['dsm_list'] = [lm_tokeniser.tokenizer.token_to_id(token)\n",
        "                            for token in lm_tokeniser.dms]\n",
        "config = hk.data_structures.to_immutable_dict(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPGaed7CisPt",
        "outputId": "e7f41d7e-c5d9-435e-c0a0-0181c2f84ff4"
      },
      "source": [
        "print(config['total_steps'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nQ2CJn5Uv0g"
      },
      "source": [
        "## Purifying the Model Functions and Getting Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijDNva7VUvV4",
        "outputId": "f6141fe3-5722-453f-f797-1f6edea01250"
      },
      "source": [
        "def featurizer(token_ids, training, config):\n",
        "    features = TransformerFeaturizer(config)(token_ids, training=training)\n",
        "    return features\n",
        "\n",
        "def logits_fn(comment_embds, comment_mask, masked_token_ids, training, config):\n",
        "    logits = ExtendedEncoder(config)(comment_embds, comment_mask, \n",
        "                                     masked_token_ids, training=training)\n",
        "    return logits\n",
        "\n",
        "key, subkey = jax.random.split( jax.random.PRNGKey(42) )\n",
        "pure_logits_fn = hk.transform(logits_fn)\n",
        "pure_featurizer_fn = hk.transform(featurizer)\n",
        "\n",
        "comment_encoding = lm_tokeniser.batch_encode_plus(['sample sentence']*config['featurizer_batch_size'])\n",
        "token_encoding = lm_tokeniser.batch_encode_plus(['sample sentence']*config['mlm_batch_size'])\n",
        "\n",
        "token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "comment_ids = np.asarray(lm_tokeniser.get_token_ids(comment_encoding), dtype=np.int16)\n",
        "\n",
        "masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWEiecuZVF_c"
      },
      "source": [
        "key, subkey = jax.random.split(key)\n",
        "featurizer_params = pure_featurizer_fn.init(subkey, comment_ids, True, config)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "comment_embds = pure_featurizer_fn.apply(featurizer_params, subkey, comment_ids, True, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq_R5RrbYSav"
      },
      "source": [
        "print(comment_embds.shape)\n",
        "print(jnp.tile(comment_embds, config['max_length']).reshape(config['mlm_batch_size'], config['max_length'], -1).shape)\n",
        "print(comment_embds.dtype, masked_token_ids.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73g9GuOaVsB5"
      },
      "source": [
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "comment_embds = jnp.tile(comment_embds, config['max_length']).reshape(config['mlm_batch_size'], config['max_length'], -1)\n",
        "comment_mask = jnp.ones_like(comment_embds[:,:,0])\n",
        "\n",
        "ExtendedEncoder_params = pure_logits_fn.init(subkey, comment_embds, \n",
        "                                             comment_mask, masked_token_ids,\n",
        "                                             True, config)\n",
        "\n",
        "params = to_immutable_dict( {'comments_encoder' : featurizer_params, \n",
        "                             'mlm_predictor' : ExtendedEncoder_params } )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyTfXPmcQ1La"
      },
      "source": [
        "pure_logits_fn.apply(params['mlm_predictor'], \n",
        "                     subkey, comment_embds, \n",
        "                     comment_mask, masked_token_ids, \n",
        "                     True, config).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnpnrl1_jEDk"
      },
      "source": [
        "@partial(jax.jit, static_argnums=(3,4))\n",
        "def pure_featurizer(params, key, token_ids, training, config):\n",
        "    key, subkey = jax.random.split(key)\n",
        "    comment_embds = pure_featurizer_fn.apply(params, subkey, comment_ids, True, config)\n",
        "    return comment_embds\n",
        "\n",
        "@partial(jax.jit, static_argnums=(5,6))\n",
        "def pure_logits(params, key, comment_embds, comment_mask, masked_token_ids, training, config):\n",
        "    key, subkey = jax.random.split(key)\n",
        "    logits = pure_logits_fn.apply(params, subkey, comment_embds, comment_mask, masked_token_ids, training=training, config=config)\n",
        "    return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2TWXXVjhQjW"
      },
      "source": [
        "## Running Model and Getting Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOnnG4nYOlL"
      },
      "source": [
        "def cross_entropy(config, original_batch, logits, masked_token_ids):\n",
        "    logits_mask = (masked_token_ids==config['mask_id'])\n",
        "    logits = jax.vmap(jnp.multiply, (None,2), 2)(logits_mask,logits)\n",
        "    labels = hk.one_hot(original_batch, config['vocab_size'])\n",
        "    softmax_xent = -jnp.sum(labels*jax.nn.log_softmax(logits))\n",
        "    total_masks = jnp.sum(logits_mask)\n",
        "    if total_masks==0:\n",
        "        return jnp.zeros(())\n",
        "    softmax_xent /= total_masks\n",
        "    return softmax_xent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgzaNYrwhTQ3"
      },
      "source": [
        "def loss(params, key, init_tree, config, turn=0):\n",
        "    \"\"\"\n",
        "    Calculates loss for all nodes of a single tree.\n",
        "    The masked tokens of each location in a comment are predicted \n",
        "    conditioned on the embeddings of all the parent comments.\n",
        "    \"\"\"\n",
        "    tree = deepcopy(init_tree)\n",
        "    loss = 0.0\n",
        "    remaining_comments = False\n",
        "\n",
        "    #Prepare embeddings of each comment\n",
        "    empty_elem = jnp.asarray([config['pad_id']]*config['max_length'], dtype=jnp.int16)\n",
        "    batches = tree_to_batch(tree, config['featurizer_batch_size'],\n",
        "                            key='tokenized_inputs', empty_elem=empty_elem)\n",
        "    \n",
        "    #print(len(batches))\n",
        "    encodings = []\n",
        "    for batch in batches:\n",
        "        key, subkey = jax.random.split(key)\n",
        "        features = pure_featurizer(params['comments_encoder'], subkey, \n",
        "                                            batch, True, config)\n",
        "        encodings.append(features)\n",
        "    tree = batch_to_tree(tree, encodings, config['featurizer_batch_size'], \n",
        "                         key='comment_embds')\n",
        "\n",
        "    #Calculate loss for each masked position in each comment.\n",
        "    comment_batches = tree_to_batch(tree, config['mlm_batch_size'], key=None, \n",
        "                                    empty_elem={}, include_root=False)\n",
        "    \n",
        "    batches = tree_to_batch(tree, config['mlm_batch_size'],\n",
        "                            key='tokenized_inputs', empty_elem=empty_elem,\n",
        "                            include_root=False)\n",
        "    \n",
        "    empty_elem = jnp.asarray([0]*config['d_model'], dtype=jnp.int16)\n",
        "    \n",
        "    for i, (original_batch, comment_batch) in enumerate( zip(batches, comment_batches) ):\n",
        "        \n",
        "        if i<turn*config['max_losses']:\n",
        "            continue\n",
        "        \n",
        "        if i==(turn+1)*config['max_losses']:\n",
        "            remaining_comments=True\n",
        "            break\n",
        "    \n",
        "        parent_comment_embds, mask_for_embds = gather_batch_parents(tree, comment_batch, \n",
        "                                                                    config['max_length'], key='comment_embds', \n",
        "                                                                    empty_elem=empty_elem)\n",
        "        key, subkey = jax.random.split(key)\n",
        "        masked_batch, original_batch = mask_batch_mlm(subkey, config, original_batch)\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        logits = pure_logits(params['mlm_predictor'], subkey, parent_comment_embds, \n",
        "                             mask_for_embds, masked_batch, True, config)\n",
        "        \n",
        "        loss += cross_entropy(config, original_batch, logits, masked_batch)\n",
        "        \n",
        "        \n",
        "    return loss, remaining_comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70kmks4YhXeM"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load Pre-trained weights\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/2SCL/Argumentation/params.pkl', 'rb') as f:\n",
        "    params = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWCmPE44hTKv"
      },
      "source": [
        "opt = get_adam_opt(config)\n",
        "opt_state = opt.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5An9g-fhaQ9"
      },
      "source": [
        "def update(opt_state, params, key, tree, config):\n",
        "    turn = 0\n",
        "    (batch_loss, remaining_comments), grad = jax.value_and_grad(loss, has_aux=True)(params, key, tree, config, turn)\n",
        "    turn += 1\n",
        "\n",
        "    while remaining_comments:\n",
        "        print(\"Big tree, turn: \", turn)\n",
        "        tup, grads = jax.value_and_grad(loss, has_aux=True)(params, key, tree, config, turn)\n",
        "        turn += 1\n",
        "        batch_loss += tup[0]\n",
        "        grad = jax.tree_util.tree_multimap(lambda x,y: x+y, grad, grads) \n",
        "        remaining_comments = tup[1]\n",
        "    \n",
        "    updates, opt_state = opt.update(grad, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)    \n",
        "    return new_params, opt_state, batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZzhHwypjQHV"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "source": [
        "import pickle"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMrXLDblhb9b",
        "outputId": "241904b7-65cb-4d74-c2ba-6e95e668b223"
      },
      "source": [
        "for epoch in range(config['n_epochs']):\n",
        "    \n",
        "    losses = []\n",
        "    for step, tree in enumerate(data_loader.tree_generator()):\n",
        "        \n",
        "        if epoch*config['n_examples']+step <= config['restart_from']:\n",
        "          continue\n",
        "        \n",
        "        if step%100==0:\n",
        "            print(f'[Step {step}]')\n",
        "\n",
        "        tree = lm_tokeniser.tokenize_tree(tree)\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        params, opt_state, batch_loss = update(opt_state, params, subkey,\n",
        "                                               tree, config)\n",
        "        #print(batch_loss)\n",
        "        losses.append(batch_loss)\n",
        "\n",
        "        if step%100==0 and step!=0:\n",
        "            print(sum(losses)/100)\n",
        "            losses = []\n",
        "\n",
        "        if step%1000==0:\n",
        "            with open(f'/content/drive/MyDrive/2SCL/Argumentation/params_{epoch}.pkl', 'wb+') as f:\n",
        "                pickle.dump(params, f)\n",
        "            print(\"Wrote params to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Evaluation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/2SCL/Argumentation/params.pkl', 'rb') as f:\n",
        "    params = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy_pred(config, original_batch, logits, masked_token_ids):\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "    logits_mask = (masked_token_ids==config['mask_id'])\n",
        "    preds = jnp.where(logits_mask, preds, -1)\n",
        "    correct_preds = jnp.sum(preds==original_batch)\n",
        "    total_preds = jnp.sum(logits_mask)\n",
        "    \n",
        "    return total_preds, correct_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(params, key, init_tree, config, turn=0):\n",
        "    \"\"\"\n",
        "    Calculates loss for all nodes of a single tree.\n",
        "    The masked tokens of each location in a comment are predicted \n",
        "    conditioned on the embeddings of all the parent comments.\n",
        "    \"\"\"\n",
        "    tree = deepcopy(init_tree)\n",
        "    total_preds = 0\n",
        "    correct_preds = 0\n",
        "    remaining_comments = False\n",
        "\n",
        "    #Prepare embeddings of each comment\n",
        "    empty_elem = jnp.asarray([config['pad_id']]*config['max_length'], dtype=jnp.int16)\n",
        "    batches = tree_to_batch(tree, config['featurizer_batch_size'],\n",
        "                            key='tokenized_inputs', empty_elem=empty_elem)\n",
        "    \n",
        "    encodings = []\n",
        "    for batch in batches:\n",
        "        key, subkey = jax.random.split(key)\n",
        "        features = pure_featurizer(params['comments_encoder'], subkey, \n",
        "                                            batch, False, config)\n",
        "        encodings.append(features)\n",
        "    tree = batch_to_tree(tree, encodings, config['featurizer_batch_size'], \n",
        "                         key='comment_embds')\n",
        "\n",
        "    #Calculate loss for each masked position in each comment.\n",
        "    comment_batches = tree_to_batch(tree, config['mlm_batch_size'], key=None, \n",
        "                                    empty_elem={}, include_root=False)\n",
        "    \n",
        "    batches = tree_to_batch(tree, config['mlm_batch_size'],\n",
        "                            key='tokenized_inputs', empty_elem=empty_elem,\n",
        "                            include_root=False)\n",
        "    \n",
        "    empty_elem = jnp.asarray([0]*config['d_model'], dtype=jnp.int16)\n",
        "    \n",
        "    for i, (original_batch, comment_batch) in enumerate( zip(batches, comment_batches) ):\n",
        "        \n",
        "        if i<turn*config['max_losses']:\n",
        "            continue\n",
        "\n",
        "        if i==(turn+1)*config['max_losses']:\n",
        "            remaining_comments=True\n",
        "            break\n",
        "     \n",
        "        parent_comment_embds, mask_for_embds = gather_batch_parents(tree, comment_batch, \n",
        "                                                                    config['max_length'], key='comment_embds', \n",
        "                                                                    empty_elem=empty_elem)\n",
        "        key, subkey = jax.random.split(key)\n",
        "        masked_batch, original_batch = mask_batch_mlm(subkey, config, original_batch)\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        logits = pure_logits(params['mlm_predictor'], subkey, parent_comment_embds, \n",
        "                             mask_for_embds, masked_batch, False, config)\n",
        "        \n",
        "        x, y = accuracy_pred(config, original_batch, logits, masked_batch)\n",
        "        total_preds += x\n",
        "        correct_preds += y\n",
        "    \n",
        "    return (total_preds, correct_preds), remaining_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tree_accuracy(params, key, tree, config):\n",
        "    turn = 0\n",
        "    total_preds, correct_preds = 0, 0\n",
        "    remaining_comments = True\n",
        "\n",
        "    while remaining_comments:\n",
        "        print(\"Big tree, turn: \", turn)\n",
        "        tup, remaining_comments = accuracy(params, key, tree, config, turn)\n",
        "        total_preds += tup[0]\n",
        "        correct_preds += tup[1]\n",
        "        turn += 1\n",
        "\n",
        "    return total_preds, correct_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_preds, correct_preds = 0, 0\n",
        "\n",
        "for step, tree in enumerate(eval_data_loader.tree_generator()):\n",
        "    if step%100==0:\n",
        "        print(f'[Step {step}]')\n",
        "\n",
        "    tree = lm_tokeniser.tokenize_tree(tree)\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    tup = tree_accuracy(params, subkey, tree, config)\n",
        "    #print(\"On tree number \", step, tup[1]/tup[0])\n",
        "    total_preds += tup[0]\n",
        "    correct_preds+= tup[1]\n",
        "\n",
        "    if step%100==0 and step!=0:\n",
        "        print(correct_preds/total_preds)"
      ]
    }
  ]
}