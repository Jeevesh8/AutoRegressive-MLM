# -*- coding: utf-8 -*-
"""AutoRegressive-MLM-ft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ZIhLlYm51O51HnTMP5ORyIO8UbMs2TP

"""

"""## AutoRegressive MLM for Argument Mining"""

import jax
import jax.numpy as jnp
import haiku as hk
from haiku.data_structures import to_immutable_dict
import optax

import copy
import numpy as np
from functools import partial
from copy import deepcopy

from src.DataLoaders.xml import load_xml_data
from src.DataLoaders.json import load_reddit_data
from src.Tokenizers.thread_tokenizer import Thread_Tokenizer
from src.model.transformer import TransformerFeaturizer, AutoRegressiveClassifier
from src.model.utils import logits_to_ar_classifier_params
from src.optimizers.adam import get_adam_opt

"""## Setting Up Config"""

root = '/content/drive/MyDrive/2SCL/Argumentation/'

config = {
          #Data Parameters
          'max_length' : 64, 
          'featurizer_batch_size' : 4,
          'mlm_batch_size' : 4,
          'n_epochs' : 10,
          'data_files' : [root+'first_batch_data/train_period_data.jsonlist'],

          'data_folders' : [root+'finetune-data/change-my-view-modes/v2.0/negative/', 
                            root+'finetune-data/change-my-view-modes/v2.0/positive/'],
                        
          'discourse_markers_file' : root+'first_batch_data/Discourse_Markers.txt',
          
          #Model Parameters
          'intermediate_size' : 2048,
          'n_heads' : 8,
          'n_layers' : 6,
          'hidden_size' : 512,
          'd_model' : 512,                                                      #same as hidden_size
          'max_losses' : 8,                                                    #max. number of losses to backpropagate at once
          'max_tree_size' : 60,
          'n_classes' :3,
          
          #Embeddings Parameters
          'embed_dropout_rate' : 0.1,
          
          #MHA parameters
          'attention_drop_rate' : 0.1,
          
          #MLP parameters
          'fully_connected_drop_rate' : 0.1,
          
          #Training Parameters
          'learning_rate' : 1e-5,
          'max_grad_norm' : 1.0,
          'l2' : 0.1,

           #colab parameter
          'restart_from' : 0,
          }

"""## Loading Data"""

data_loader = load_reddit_data(config)

train_data_loader = load_xml_data(config, split='train/')

valid_data_loader = load_xml_data(config, split='valid/')

"""## Training Tokenizer

"""

def get_sentences():
    for tree in data_loader.tree_generator():
        yield tree['title'] + ' ' + tree['selftext']
        for id, comment in tree['comments'].items():
            yield comment['body']

lm_tokeniser = Thread_Tokenizer(config)
lm_tokeniser.train_tokenizer(str_iter=get_sentences())

print(lm_tokeniser.tokenizer.get_vocab(), lm_tokeniser.tokenizer.id_to_token(3301))

"""### Updating Config"""

config['vocab_size'] = lm_tokeniser.tokenizer.get_vocab_size()

#Tokenization ids  
config['mask_id'] = lm_tokeniser.tokenizer.token_to_id("<mask>")
config['pad_id'] = lm_tokeniser.tokenizer.token_to_id("<pad>")
config['sos_id'] = lm_tokeniser.tokenizer.token_to_id("<s>")
config['eos_id'] = lm_tokeniser.tokenizer.token_to_id("</s>")
config['total_steps'] = len([0 for thread in train_data_loader.thread_generator()])
config['dsm_list'] = [lm_tokeniser.tokenizer.token_to_id(token)
                            for token in lm_tokeniser.dms]
config = hk.data_structures.to_immutable_dict(config)

print(config['total_steps'])

"""## Purifying the Model Functions and Getting Parameters"""

def featurizer(token_ids, training, config):
    features = TransformerFeaturizer(config)(token_ids, training=training)
    return features

def logits_fn(comment_embds, comment_mask, masked_token_ids, training, config):
    logits = AutoRegressiveClassifier(config)(comment_embds, comment_mask, 
                                              masked_token_ids, training=training)
    return logits

key, subkey = jax.random.split( jax.random.PRNGKey(42) )
pure_logits_fn = hk.transform(logits_fn)
pure_featurizer_fn = hk.transform(featurizer)

comment_encoding = lm_tokeniser.batch_encode_plus(['sample sentence'+' <pad> '*(config['max_length']-2)]
                                                  *config['featurizer_batch_size'])
token_encoding = lm_tokeniser.batch_encode_plus(['sample sentence'+ ' <pad> '*(config['max_length']-2)]
                                                 *config['mlm_batch_size'])

token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)
comment_ids = np.asarray(lm_tokeniser.get_token_ids(comment_encoding), dtype=np.int16)

masked_token_ids, original_batch = token_ids, token_ids

key, subkey = jax.random.split(key)
featurizer_params = pure_featurizer_fn.init(subkey, comment_ids, True, config)

key, subkey = jax.random.split(key)
comment_embds = pure_featurizer_fn.apply(featurizer_params, subkey, comment_ids, True, config)

print(comment_embds.shape)
print(jnp.tile(comment_embds, config['max_length']).reshape(config['mlm_batch_size'], config['max_length'], -1).shape)
print(comment_embds.dtype, masked_token_ids.dtype)

key, subkey = jax.random.split(key)

comment_embds = jnp.tile(comment_embds, config['max_length']).reshape(config['mlm_batch_size'], config['max_length'], -1)
comment_mask = jnp.ones_like(comment_embds[:,:,0])

ExtendedEncoder_params = pure_logits_fn.init(subkey, comment_embds, 
                                             comment_mask, masked_token_ids,
                                             True, config)

params = {'comments_encoder' : featurizer_params, 
          'ar_classifier' : ExtendedEncoder_params }

#Load Pre-trained weights
import pickle
with open('/content/drive/MyDrive/2SCL/Argumentation/long_train_params.pkl', 'rb') as f:
    pretrained_params = pickle.load(f)

params = logits_to_ar_classifier_params(pretrained_params, params)

pure_logits_fn.apply(params['ar_classifier'], 
                     subkey, comment_embds, 
                     comment_mask, masked_token_ids, 
                     True, config).shape

@partial(jax.jit, static_argnums=(3,4))
def pure_featurizer(params, key, token_ids, training, config):
    key, subkey = jax.random.split(key)
    comment_embds = pure_featurizer_fn.apply(params, subkey, comment_ids, True, config)
    return comment_embds

@partial(jax.jit, static_argnums=(5,6))
def pure_logits(params, key, comment_embds, comment_mask, masked_token_ids, training, config):
    key, subkey = jax.random.split(key)
    logits = pure_logits_fn.apply(params, subkey, comment_embds, comment_mask, masked_token_ids, training=training, config=config)
    return logits

"""## Running Model and Getting Loss"""

def cross_entropy(config, original_batch, logits, masked_token_ids):
    logits_mask = (masked_token_ids!=config['pad_id'])
    logits = jax.vmap(jnp.multiply, (None,2), 2)(logits_mask,logits)
    labels = hk.one_hot(original_batch, config['n_classes'])
    softmax_xent = -jnp.sum(labels*jax.nn.log_softmax(logits))
    total_masks = jnp.sum(logits_mask)
    if total_masks==0:
        return jnp.zeros(())
    softmax_xent /= total_masks
    return softmax_xent

def get_batched_version(elem_lis, batch_size, empty_elem):
    extra = len(elem_lis)%batch_size
    
    if extra!=0:
        elem_lis += [empty_elem]*(batch_size-extra)
    
    return [ jnp.stack(elem_lis[i*batch_size : (i+1)*batch_size]) 
             for i in range(len(elem_lis)//batch_size) ]

def loss(params, key, init_thread, config, turn=0):
    """
    init_thread:  list of jnp.arrays having token ids.
    """
    thread = deepcopy(init_thread[0])
    labels = deepcopy(init_thread[1])
    #print(len(thread), len(labels), thread[0].shape)
    loss = 0.0
    remaining_comments = False

    empty_elem = jnp.asarray([config['pad_id']]*config['max_length'], dtype=jnp.int16)
    
    batches = get_batched_version(thread, config['featurizer_batch_size'], empty_elem)
    #print(len(batches), batches[0].shape)
    encodings = []
    for batch in batches:
        key, subkey = jax.random.split(key)
        features = pure_featurizer(params['comments_encoder'], subkey, 
                                            batch, True, config)
        encodings+=[elem for elem in features]
    #print(len(encodings))
    parent_encodings = [jnp.stack( encodings[:i]+[ jnp.zeros_like(encodings[0]) ]*(config['max_length']-i) ) 
                        for i in range(min(len(encodings), config['max_length']))]
    
    parent_mask_lis = [jnp.asarray( [0]*i +[1]*(config['max_length']-i) , dtype=jnp.int16) 
                        for i in range(min(len(encodings), config['max_length']))]
    #print(parent_encodings[1],parent_encodings[1].shape, len(parent_encodings), len(parent_mask_lis))
    parent_encodings = get_batched_version(parent_encodings, config['featurizer_batch_size'], parent_encodings[0])
    parent_mask_lis = get_batched_version(parent_mask_lis, config['featurizer_batch_size'], parent_mask_lis[0])
    label_batches = get_batched_version(labels, config['featurizer_batch_size'], labels[0])
    #print(len(parent_encodings), len(parent_mask_lis), len(batches), len(label_batches))

    for i, (batch, parent_batch, parent_mask, labels) in enumerate( zip(batches, parent_encodings, parent_mask_lis, label_batches) ):
        #print(batch.shape, parent_batch.shape, parent_mask.shape, labels.shape)
        if i<turn*config['max_losses']:
            continue
        
        if i==(turn+1)*config['max_losses']:
            remaining_comments=True
            break
    
        key, subkey = jax.random.split(key)
        logits = pure_logits(params['ar_classifier'], subkey, parent_batch, 
                             parent_mask, batch, True, config)
        
        loss += cross_entropy(config, labels, logits, batch)
        
        
    return loss, remaining_comments

"""## Optimizer"""

opt = get_adam_opt(config)
opt_state = opt.init(params)

def update(opt_state, params, key, thread, config):
    turn = 0
    (batch_loss, remaining_comments), grad = jax.value_and_grad(loss, has_aux=True)(params, key, thread, config, turn)
    turn += 1

    while remaining_comments:
        print("Big tree, turn: ", turn)
        tup, grads = jax.value_and_grad(loss, has_aux=True)(params, key, tree, config, turn)
        turn += 1
        batch_loss += tup[0]
        grad = jax.tree_util.tree_multimap(lambda x,y: x+y, grad, grads) 
        remaining_comments = tup[1]
    
    updates, opt_state = opt.update(grad, opt_state)
    new_params = optax.apply_updates(params, updates)    
    return new_params, opt_state, batch_loss

"""## Training Loop"""

import pickle

for _ in range(config['n_epochs']):
    
    losses = []
    for step, thread in enumerate(train_data_loader.thread_generator()):
        #print(thread)
        if _*config['total_steps']+step <= config['restart_from']:
          continue
        
        if step%(config['total_steps']//3)==0:
            print(f'[Step {step}]')

        thread = lm_tokeniser.tokenize_thread(thread)
        #print([elem.shape for elem in thread[0]])
        #print([elem.shape for elem in thread[1]])
        key, subkey = jax.random.split(key)
        params, opt_state, batch_loss = update(opt_state, params, subkey,
                                               thread, config)
        #print(batch_loss)
        losses.append(batch_loss)

        if step%(config['total_steps']//3)==0:
            print(sum(losses)/len(losses))
            losses = []

        if step==config['total_steps']-1:
            with open('/content/drive/MyDrive/2SCL/Argumentation/finetune_params.pkl', 'wb+') as f:
                pickle.dump(params, f)
            print("Wrote params to disk")

"""## Evaluation"""

import pickle
with open('/content/drive/MyDrive/2SCL/Argumentation/finetune_params.pkl', 'rb') as f:
    params = pickle.load(f)

def accuracy_pred(config, labels, logits, batch):
    preds = jnp.argmax(logits, axis=-1)
    logits_mask = (batch!=config['pad_id'])
    preds = jnp.where(logits_mask, preds, -1)
    print("Predictions: ", preds)
    print("labels: ",  labels)
    correct_preds = jnp.sum(preds==labels)
    total_preds = jnp.sum(logits_mask)
    return total_preds, correct_preds

def accuracy(params, key, init_thread, config, turn=0):
    """
    Calculates loss for all nodes of a single tree.
    The masked tokens of each location in a comment are predicted 
    conditioned on the embeddings of all the parent comments.
    """
    thread = deepcopy(init_thread[0])
    labels = deepcopy(init_thread[1])
    #print(thread, labels)
    total_preds = 0
    correct_preds = 0
    remaining_comments = False

    #Prepare embeddings of each comment
    empty_elem = jnp.asarray([config['pad_id']]*config['max_length'], dtype=jnp.int16)
    batches = get_batched_version(thread, config['featurizer_batch_size'], empty_elem)

    encodings = []
    for batch in batches:
        key, subkey = jax.random.split(key)
        features = pure_featurizer(params['comments_encoder'], subkey, 
                                            batch, False, config)
        encodings+=[elem for elem in features]
    
    parent_encodings = [jnp.stack( encodings[:i]+[ jnp.zeros_like(encodings[0]) ]*(config['max_length']-i) ) 
                        for i in range(min(len(encodings), config['max_length']))]
    
    parent_mask_lis = [jnp.asarray( [0]*i +[1]*(config['max_length']-i) , dtype=jnp.int16) 
                        for i in range(min(len(encodings), config['max_length']))]
    
    parent_encodings = get_batched_version(parent_encodings, config['featurizer_batch_size'], parent_encodings[0])
    parent_mask_lis = get_batched_version(parent_mask_lis, config['featurizer_batch_size'], parent_mask_lis[0])
    label_batches = get_batched_version(labels, config['featurizer_batch_size'], labels[0])
    
    for i, (batch, parent_batch, parent_mask, labels) in enumerate( zip(batches, parent_encodings, parent_mask_lis, label_batches) ):
        
        if i<turn*config['max_losses']:
            continue
        
        if i==(turn+1)*config['max_losses']:
            remaining_comments=True
            break
    
        key, subkey = jax.random.split(key)
        logits = pure_logits(params['ar_classifier'], subkey, parent_batch, 
                             parent_mask, batch, True, config)
        
        x, y = accuracy_pred(config, labels, logits, batch)
        total_preds += x
        correct_preds += y
    
    return (total_preds, correct_preds), remaining_comments

def thread_accuracy(params, key, thread, config):
    turn = 0
    total_preds, correct_preds = 0, 0
    remaining_comments = True

    while remaining_comments:
        print("Big tree, turn: ", turn)
        tup, remaining_comments = accuracy(params, key, thread, config, turn)
        total_preds += tup[0]
        correct_preds += tup[1]
        turn += 1

    return total_preds, correct_preds

total_preds, correct_preds = 0, 0

for step, thread in enumerate(valid_data_loader.thread_generator()):
    if step%100==0:
        print(f'[Step {step}]')
    print(thread)
    thread = lm_tokeniser.tokenize_thread(thread)

    key, subkey = jax.random.split(key)
    tup = thread_accuracy(params, subkey, thread, config)
    #print("On tree number ", step, tup[1]/tup[0])
    total_preds += tup[0]
    correct_preds+= tup[1]
    print(correct_preds/total_preds)

